\section{Conclusion \& Outlook}
\label{sec:conclusion}

We presented a method to calculate similarity between words based on their phonetic \gls{ipa} transcription, \ie how they sound when spoken. For this task, the Needleman-Wunsch algorithm with similarity matrix and gap penalty was employed. We implemented this algorithm in Rust and parallelized it on a CPU using the \textit{Rayon} library and on a consumer Nvidia GPU using the CUDA framework with the \textit{cudarc} Rust library, while writing the kernel itself in \Cpp. We detailed choice of data structures, indexing and memory layout to optimize the performance of the GPU implementation. For a graph with 100,000 nodes (words) and almost 5 billion edges (word-pairs), the algorithm takes less than \qty{20}{\s} (including copying back to host). Consistency between the CPU and GPU implementations was verified up to 30,000 nodes (after that, the CPU implementation becomes too slow and consumes too much memory).

Future work could include an adapted version that can read the whole graph of more than 600,000 nodes at the same time by copying back intermediate results from the GPU to the CPU as outlined in the evaluation. To further speed up the computation, one could consider a more fine-grained parallelization at the level of the score matrix calculation. This is not trivial due to the dependencies between the cells of the matrix. A stencil computation approach might be feasible.

We also demonstrated the practical usability of the resulting edge weights by examining the graphs in Gephi. Even just by looking at small subsections of the full graph, we can deduce interesting properties of the language, \eg community detection revealed groups with similar word endings and groups with the same root but different endings. Constructing ego-networks allows to find words that are phonetically similar to a given word. One can envision an online platform where this functionality is offered to users to find rhymes or similar sounding words. This can be helpful for language learners as well to foster the playful exploration of a language. Beyond that, future work might tackle the following points:

\begin{itemize}
    \item Fine-tune the similarity matrix for language subtleties. In this document, we always used a fixed match/mismatch score of 1/-1, while in reality, some phonetic substitutions sound more similar than others. For example, replacing a vowel by a consonant might be more severe than replacing a vowel by another vowel. One might also want to experiment with the gap penalty that was set to $-1$ throughout.
    
    \item So far, we considered a dataset for the French language, while the presented pipeline can work with other languages as well. This can open up the possibility to compare the phonetic structure of different languages and find similarities between them.
    
    \item The Louvain algorithm \cite{louvain} for community detection inherently constructs a multi-level hierarchy of communities. Looking at a lower granularity level (more high-level view) could reveal interesting results. For this purpose, we should consider a parallel Louvain implementation that can work on the whole graph. Limiting oneself to a slice of edge weights (as done here) might bias results or leave interesting structures unnoticed.
    
    \item The size of the dataset can be reduced by merging words with the same lexeme, \eg for \textit{remportés}, \textit{remportant}, \textit{remportée}, \textit{remportées} etc. we could only consider the lexeme \textit{remporter}. This could help gain a better and more general understanding of the phonetic structure of the language as the graphs would not be cluttered with variations of the same word.
\end{itemize}
